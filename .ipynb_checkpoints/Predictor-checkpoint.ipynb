{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Y_train.txt', 'X_train.txt', 'activity_labels.txt', 'sample_submission.txt', 'X_test.txt', 'features.txt', 'features_info.txt', 'README.txt']\n",
      "Imports Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"input/\"))\n",
    "\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot\n",
    "import xgboost as xgb\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Imports Complete\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading X\n",
      "        0         1         2         3         4         5         6    \\\n",
      "0  0.271925 -0.017309 -0.106774 -0.982197 -0.986899 -0.967111 -0.983637   \n",
      "1  0.335256  0.019098 -0.055751  0.080347  0.096305 -0.136543 -0.049000   \n",
      "2  0.131517 -0.115616 -0.208167 -0.246124 -0.274395 -0.059285 -0.326047   \n",
      "3  0.279240 -0.018476 -0.105920 -0.984630 -0.966410 -0.952940 -0.985880   \n",
      "4  0.285120  0.006762 -0.087324 -0.417612 -0.093410 -0.654948 -0.425524   \n",
      "\n",
      "        7         8         9      ...          551       552       553  \\\n",
      "0 -0.986711 -0.963578 -0.925383    ...     0.153164 -0.438706 -0.767483   \n",
      "1 -0.044129 -0.248500  0.483926    ...    -0.144744 -0.029993 -0.471274   \n",
      "2 -0.310820 -0.023570  0.041278    ...     0.376784 -0.588954 -0.786680   \n",
      "3 -0.972560 -0.950020 -0.930450    ...     0.218200 -0.213440 -0.596880   \n",
      "4 -0.090418 -0.646486 -0.295091    ...     0.205700 -0.397920 -0.739947   \n",
      "\n",
      "        554       555       556       557       558       559       560  \n",
      "0  0.138323  0.468688 -0.641037 -0.186480  0.539115 -0.844479  0.180127  \n",
      "1 -0.051427  0.816770  0.815910  0.517587 -0.641386  0.222501  0.239129  \n",
      "2  0.059287  0.594489  0.985875 -0.490529 -0.601561  0.261179  0.244828  \n",
      "3  0.148710 -0.158230 -0.799410  0.336120 -0.443530 -0.078807  0.367280  \n",
      "4 -0.070058 -0.032825  0.663856  0.762120 -0.891179  0.126266 -0.056026  \n",
      "\n",
      "[5 rows x 561 columns]\n",
      "(9625, 561)\n",
      "loading XFinal\n",
      "loading Y\n",
      "   0\n",
      "0  6\n",
      "1  3\n",
      "2  2\n",
      "3  4\n",
      "4  1\n",
      "(9625, 1)\n",
      "Converting y to Categorical Vector\n",
      "[[0 0 0 0 0 0 1]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0]]\n",
      "(9625, 7)\n",
      "Data Loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"loading X\")\n",
    "X = pd.read_csv('input/X_train.txt', sep='\\s+',header=None)\n",
    "X_original=X\n",
    "print(X.head())\n",
    "print(X.shape)\n",
    "\n",
    "print(\"loading XFinal\")\n",
    "final_X = pd.read_csv('input/X_test.txt', sep='\\s+',header=None)\n",
    "final_X_original=final_X\n",
    "\n",
    "\n",
    "print(\"loading Y\")\n",
    "Y = pd.read_csv('input/Y_train.txt', sep='\\s+',header=None)\n",
    "Y = Y.astype(int)\n",
    "print(Y.head())\n",
    "print(Y.shape)\n",
    "\n",
    "print(\"Converting y to Categorical Vector\")\n",
    "Y_original = Y\n",
    "#original Y saved in Y_original\n",
    "Y = keras.utils.to_categorical(Y,dtype='int32')\n",
    "print(Y[:8])\n",
    "print(Y.shape)\n",
    "\n",
    "print(\"Data Loaded\")\n",
    "X_original=X\n",
    "X = X.values\n",
    "final_X = final_X.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Train-Test\n",
      "trainX shape =  (7700, 561) trainY shape =  (7700, 1)\n",
      "testX shape =  (1925, 561) testY shape =  (1925, 1)\n",
      "finalX shape =  (6418, 561)\n",
      "Finding Feature Importances\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 11 ne= 200 Accuracy =  95.58441558441558 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 11 ne= 300 Accuracy =  95.84415584415584 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 11 ne= 400 Accuracy =  95.94805194805195 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 11 ne= 500 Accuracy =  96.05194805194806 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 11 ne= 600 Accuracy =  96.0 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 11 ne= 700 Accuracy =  96.0 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 11 ne= 800 Accuracy =  96.0 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 11 ne= 900 Accuracy =  96.1038961038961 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 11 ne= 1000 Accuracy =  96.15584415584416 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 11 ne= 1100 Accuracy =  96.15584415584416 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 11 ne= 1200 Accuracy =  96.15584415584416 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 21 ne= 200 Accuracy =  95.58441558441558 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 21 ne= 300 Accuracy =  95.53246753246754 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 21 ne= 400 Accuracy =  95.63636363636364 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 21 ne= 500 Accuracy =  95.74025974025973 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 21 ne= 600 Accuracy =  95.94805194805195 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 21 ne= 700 Accuracy =  95.94805194805195 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 21 ne= 800 Accuracy =  96.0 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 21 ne= 900 Accuracy =  96.0 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 21 ne= 1000 Accuracy =  96.0 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 21 ne= 1100 Accuracy =  96.05194805194806 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 21 ne= 1200 Accuracy =  96.1038961038961 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 31 ne= 200 Accuracy =  95.63636363636364 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 31 ne= 300 Accuracy =  95.8961038961039 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 31 ne= 400 Accuracy =  96.05194805194806 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 31 ne= 500 Accuracy =  96.05194805194806 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 31 ne= 600 Accuracy =  96.05194805194806 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 31 ne= 700 Accuracy =  96.05194805194806 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 31 ne= 800 Accuracy =  96.0 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 31 ne= 900 Accuracy =  96.05194805194806 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 31 ne= 1000 Accuracy =  95.94805194805195 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 31 ne= 1100 Accuracy =  95.94805194805195 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 31 ne= 1200 Accuracy =  96.0 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 41 ne= 200 Accuracy =  95.63636363636364 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 41 ne= 300 Accuracy =  95.84415584415584 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 41 ne= 400 Accuracy =  95.84415584415584 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 41 ne= 500 Accuracy =  95.84415584415584 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 41 ne= 600 Accuracy =  95.94805194805195 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 41 ne= 700 Accuracy =  96.0 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 41 ne= 800 Accuracy =  96.0 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 41 ne= 900 Accuracy =  96.0 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 41 ne= 1000 Accuracy =  96.0 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 41 ne= 1100 Accuracy =  96.05194805194806 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 41 ne= 1200 Accuracy =  96.05194805194806 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 51 ne= 200 Accuracy =  95.63636363636364 %\n",
      "\n",
      "Fitting\n",
      "\n",
      "md= 51 ne= 300 Accuracy =  95.84415584415584 %\n",
      "\n",
      "Fitting\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-49deadc0f3c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# fit model no training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_child_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubsample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m# plot feature importance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#plot_importance(f_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    711\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1110\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Splitting Train-Test\")\n",
    "trainX, testX, trainY, testY = train_test_split(X_original, Y_original, test_size=0.2)\n",
    "print (\"trainX shape = \",trainX.shape,\"trainY shape = \", trainY.shape)\n",
    "print (\"testX shape = \", testX.shape, \"testY shape = \", testY.shape)\n",
    "print( \"finalX shape = \", final_X_original.shape)\n",
    "print(\"Finding Feature Importances\\n\")\n",
    "acc=0.0\n",
    "for md in range(11,100,10):\n",
    "    for ne in range(200,1201,100):\n",
    "        print(\"Fitting\\n\")\n",
    "        # fit model no training data\n",
    "        f_model = XGBClassifier(max_depth=md, n_estimators=ne, n_jobs=3, verbosity=2, min_child_weight=1,subsample=1, random_state=0)\n",
    "        f_model.fit(trainX, np.ravel(trainY))\n",
    "        # plot feature importance\n",
    "        #plot_importance(f_model)\n",
    "        #pyplot.show()\n",
    "        # plot feature importance using built-in function\n",
    "        predY = f_model.predict(testX)\n",
    "        #predY = np.argmax(predY, axis = 1)\n",
    "        predY=predY.astype('int32')\n",
    "        #print(\"Predictions : \")\n",
    "        #print(\"shape = \" , predY.shape)\n",
    "        #print(predY[:8])\n",
    "        #print(\"Truth : \")\n",
    "        #testY = np.argmax(testY, axis = 1)\n",
    "        testY = testY.astype('int32')\n",
    "        #print(\"shape = \" , testY.shape)\n",
    "        #print(testY[:8])\n",
    "\n",
    "        acc = sklearn.metrics.accuracy_score(testY,predY,normalize='True') * 100\n",
    "        print(\"md=\",md,\"ne=\",ne,\"Accuracy = \",acc, \"%\\n\")\n",
    "        \n",
    "        \n",
    "        if (acc)>97.0:\n",
    "            joblib.dump(f_model, str(md)+str(ne)+'_'+str(acc)+\".joblib.dat\")\n",
    "            f_model = joblib.load(str(md)+str(ne)+'_'+str(acc)+\".joblib.dat\")\n",
    "            final_predY = f_model.predict(final_X_original)\n",
    "            #final_predY = np.argmax(final_predY, axis = 1)\n",
    "            final_predY=final_predY.astype('int32')\n",
    "            for i in range(len(final_predY)):\n",
    "                print(str(i)+\",\"+str(final_predY[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Splitting Train-Test\")\n",
    "trainX, testX, trainY, testY = train_test_split(X_original, Y_original, test_size=0.2)\n",
    "print (\"trainX shape = \",trainX.shape,\"trainY shape = \", trainY.shape)\n",
    "print (\"testX shape = \", testX.shape, \"testY shape = \", testY.shape)\n",
    "print( \"finalX shape = \", final_X_original.shape)\n",
    "print(\"Finding Feature Importances\\n\")\n",
    "acc=0.0\n",
    "for md in range(1,42,10):\n",
    "    for ne in range(150,1401,100):\n",
    "        print(\"Splitting Train-Test\")\n",
    "        trainX, testX, trainY, testY = train_test_split(X_original, Y_original, test_size=0.2)\n",
    "        print(\"Fitting\\n\")\n",
    "        # fit model no training data\n",
    "        f_model = XGBClassifier(max_depth=md, n_estimators=ne, n_jobs=3, verbosity=2, min_child_weight=1,subsample=1, random_state=0)\n",
    "        f_model.fit(trainX, np.ravel(trainY))\n",
    "        # plot feature importance\n",
    "        #plot_importance(f_model)\n",
    "        #pyplot.show()\n",
    "        # plot feature importance using built-in function\n",
    "        predY = f_model.predict(testX)\n",
    "        #predY = np.argmax(predY, axis = 1)\n",
    "        predY=predY.astype('int32')\n",
    "        #print(\"Predictions : \")\n",
    "        #print(\"shape = \" , predY.shape)\n",
    "        #print(predY[:8])\n",
    "        #print(\"Truth : \")\n",
    "        #testY = np.argmax(testY, axis = 1)\n",
    "        testY = testY.astype('int32')\n",
    "        #print(\"shape = \" , testY.shape)\n",
    "        #print(testY[:8])\n",
    "\n",
    "        acc = sklearn.metrics.accuracy_score(testY,predY,normalize='True') * 100\n",
    "        print(\"md=\",md,\"ne=\",ne,\"Accuracy = \",acc, \"%\\n\")\n",
    "        \n",
    "        \n",
    "        if (acc)>97.0:\n",
    "            joblib.dump(f_model, str(md)+str(ne)+'_'+str(acc)+\".joblib.dat\")\n",
    "            f_model = joblib.load(str(md)+str(ne)+'_'+str(acc)+\".joblib.dat\")\n",
    "            final_predY = f_model.predict(final_X_original)\n",
    "            #final_predY = np.argmax(final_predY, axis = 1)\n",
    "            final_predY=final_predY.astype('int32')\n",
    "            for i in range(len(final_predY)):\n",
    "                print(str(i)+\",\"+str(final_predY[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
